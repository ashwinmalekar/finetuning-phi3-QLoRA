# -*- coding: utf-8 -*-
"""phi-3_SQuAD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jpRl8Eyqdtb_oVdPkOeWUtdVVAr-2NOY

**FINE-TUNING Phi-3 on SQuAD**
"""

!pip install -q transformers datasets peft accelerate bitsandbytes trl torch

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline
)

from peft import LoraConfig, get_peft_model, TaskType, PeftModel
from datasets import load_dataset
from trl import SFTTrainer
import os

# Disable wandb completely
os.environ["WANDB_DISABLED"] = "true"

print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}")

#CONFIG

MODEL_NAME = "microsoft/Phi-3-mini-4k-instruct"

# dataset config
DATASET_NAME = "squad_v2"
TRAIN_SAMPLES = 50

# training config
OUTPUT_DIR = "./qlora-finetuned-model_3"

#quantization config

# 4-bit quantization config for memory efficiency
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

# load model & tokenizer

print(f"Loading model: {MODEL_NAME}")

# load tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# load model with quantization config
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
)

from peft import prepare_model_for_kbit_training
model = prepare_model_for_kbit_training(model)


model.gradient_checkpointing_enable()

# LoRA config
lora_config = LoraConfig(
    r=16,  # rank
    lora_alpha=32,
    target_modules=[
        "qkv_proj", "o_proj", "gate_up_proj", "down_proj"
    ],  # Phi-3 specific target modules
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.CAUSAL_LM,
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

#dataset prep
print(f"Loading dataset: {DATASET_NAME}")

# load SQuAD v2 with 500 samples
dataset = load_dataset("squad_v2", split=f"train[:{TRAIN_SAMPLES}]")

def format_squad_examples(example):
    """format squad_v2 for phi-3 instruction"""
    context = example["context"]
    question = example["question"]
    answers = example["answers"]["text"]

    if len(answers) > 0:
        answer = answers[0]
    else:
        answer = "No answer available"

    # Phi-3 instruction format
    prompt = f"""<|user|>
Context: {context}

Question: {question}<|end|>
<|assistant|>
{answer}<|end|>"""

    return {"text": prompt}

dataset = dataset.map(format_squad_examples)

def tokenize_function(example):
    tokenized = tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",
        max_length=128,
    )
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

dataset = dataset.map(tokenize_function, batched=False)

#train_test split
train_dataset = dataset.train_test_split(test_size=0.1, seed=42)["train"]
eval_dataset = dataset.train_test_split(test_size=0.1, seed=42)["test"]

print(f"train samples: {len(train_dataset)}")
print(f"eval samples: {len(eval_dataset)}")

print("\nExample formatted prompt:")
print(train_dataset[0]["text"][:500] + "...")

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=3,  # more epochs as training on few samples
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=1,
    optim="paged_adamw_8bit",
    save_steps=20,  # more frequent saves for smaller dataset
    logging_steps=10,  # more frequent logging
    learning_rate=2e-4,
    weight_decay=0.001,
    fp16=False,
    bf16=True,
    max_grad_norm=0.3,
    max_steps=10,
    warmup_ratio=0.05,
    group_by_length=True,
    lr_scheduler_type="constant",
    report_to=[],
    eval_strategy="steps",
    eval_steps=20,  # more frequent eval
    save_strategy="steps",
    load_best_model_at_end=False,
    push_to_hub=False,
    remove_unused_columns=False,
    run_name=None
)

trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    peft_config=lora_config,
    processing_class=tokenizer,
    args=training_args,
)

print("Starting training...")

# Train the model
trainer.train()

# Save the fine-tuned model
trainer.model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print(f"Training completed! Model saved to {OUTPUT_DIR}")

base_model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
)

# Load the fine-tuned LoRA weights
model = PeftModel.from_pretrained(base_model, "./qlora-finetuned-model_2")

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token



test_prompts = [
    """<|user|>
Context: The Amazon rainforest is the world's largest tropical rainforest. It covers much of the Amazon Basin of South America. This basin encompasses 7,000,000 square kilometers, of which 5,500,000 square kilometers are covered by the rainforest.

Question: How large is the Amazon Basin?<|end|>
<|assistant|>""",

    """<|user|>
Context: Python is a high-level programming language. It was created by Guido van Rossum and first released in 1991. Python emphasizes code readability with its notable use of significant whitespace.

Question: Who created Python?<|end|>
<|assistant|>"""
]

print("\nTesting fine-tuned model:")
for i, prompt in enumerate(test_prompts, 1):
    print(f"\n--- Test {i} ---")
    print("Input:", prompt.split("Question: ")[1].split("<|end|>")[0])

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
            use_cache=False
        )

    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
    assistant_response = generated_text.split("<|assistant|>")[-1].strip()
    print("Answer:", assistant_response)

